{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:  calculate the tf-idf for the movie_review corpus and the top ranking words for that corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moviedir = r'C:\\Users\\Aether Analytics\\Desktop\\movie_reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading all files as training data. \n",
    "movie_train = load_files(moviedir, shuffle=True)\n",
    "len(movie_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target names (\"classes\") are automatically generated from subfolder names\n",
    "movie_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so cal\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First file seems to be about a Schwarzenegger movie. \n",
    "movie_train.data[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Aether Analytics\\\\Desktop\\\\movie_reviews\\\\neg\\\\cv405_21868.txt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first file is in \"neg\" folder\n",
    "movie_train.filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #first file is a negative review and is mapped to 0 index 'neg' in target_names\n",
    "movie_train.target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer & TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "# Turn off pretty printing of jupyter notebook... it generates long lines\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = ['A rose is a rose is a rose is a rose.',\n",
    "         'Oh, what a fine day it is.',\n",
    "        \"It ain't over till it's over, I tell you!!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a CoutVectorizer to use NLTK's tokenizer instead of its \n",
    "# default one (which ignores punctuation and stopwords). \n",
    "# Minimum document frequency set to 1. \n",
    "foovec = CountVectorizer(min_df=1, tokenizer=nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'!': 0, u\"'s\": 1, u'ai': 5, u'over': 13, u'it': 10, u'fine': 7, u'day': 6, u'a': 4, u'what': 17, u'oh': 12, u'i': 8, u'rose': 14, u',': 2, u'.': 3, u\"n't\": 11, u'till': 16, u'tell': 15, u'you': 18, u'is': 9}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sents turned into sparse vector of word frequency counts\n",
    "sents_counts = foovec.fit_transform(sents)\n",
    "# foovec now contains vocab dictionary which maps unique words to indexes\n",
    "foovec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 19)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sents_counts has a dimension of 3 (document count) by 19 (# of unique words)\n",
    "sents_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 4, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [2, 1, 1, 0, 0, 1, 0, 0, 1, 0, 2, 1, 0, 2, 0, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this vector is small enough to view in full! \n",
    "sents_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF (Term Frequency -- Inverse Document Frequency) values\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "sents_tfidf = tfidf_transformer.fit_transform(sents_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.13650997,  0.54603988,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.40952991,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.71797683,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.28969526,  0.28969526,  0.28969526,\n",
       "         0.        ,  0.38091445,  0.38091445,  0.        ,  0.28969526,\n",
       "         0.28969526,  0.        ,  0.38091445,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.38091445,  0.        ],\n",
       "       [ 0.47282517,  0.23641258,  0.17979786,  0.        ,  0.        ,\n",
       "         0.23641258,  0.        ,  0.        ,  0.23641258,  0.        ,\n",
       "         0.35959573,  0.23641258,  0.        ,  0.47282517,  0.        ,\n",
       "         0.23641258,  0.23641258,  0.        ,  0.23641258]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF values\n",
    "# raw counts have been normalized against document length, \n",
    "# terms that are found across many docs are weighted down\n",
    "sents_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transforming movie reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize movie_vector object, and then turn movie train data into a vector \n",
    "movie_vec = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize)         # use all 25K words. 82.2% acc.\n",
    "# movie_vec = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features = 3000) # use top 3000 words only. 78.5% acc.\n",
    "movie_counts = movie_vec.fit_transform(movie_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19637"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'screen' is found in the corpus, mapped to index 19637\n",
    "movie_vec.vocabulary_.get('screen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19690"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Likewise, Mr. Steven Seagal is present...\n",
    "movie_vec.vocabulary_.get('seagal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 25313)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# huge dimensions! 2,000 documents, 25K unique terms. \n",
    "movie_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "movie_tfidf = tfidf_transformer.fit_transform(movie_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 25313)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same dimensions, now with tf-idf values instead of raw frequency counts\n",
    "movie_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and testing a Naive Bayes classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now ready to build a classifier. \n",
    "# We will use Multinominal Naive Bayes as our model\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "# from sklearn.cross_validation import train_test_split  # deprecated in 0.18\n",
    "from sklearn.model_selection import train_test_split\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    movie_tfidf, movie_train.target, test_size = 0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a Multimoda Naive Bayes classifier\n",
    "clf = MultinomialNB().fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82250000000000001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results, find accuracy\n",
    "y_pred = clf.predict(docs_test)\n",
    "sklearn.metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[176,  30],\n",
       "       [ 41, 153]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying classifier on fake movie reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# very short and fake movie reviews\n",
    "reviews_new = ['This movie was excellent', 'Absolute joy ride', \n",
    "            'Steven Seagal was terrible', 'Steven Seagal shined through.', \n",
    "              'This was certainly a movie', 'Two thumbs up', 'I fell asleep halfway through', \n",
    "              \"We can't wait for the sequel!!\", '!', '?', 'I cannot recommend this highly enough', \n",
    "              'instant classic.', 'Steven Seagal was amazing. His performance was Oscar-worthy.']\n",
    "reviews_new_counts = movie_vec.transform(reviews_new)\n",
    "reviews_new_tfidf = tfidf_transformer.transform(reviews_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# have classifier make a prediction\n",
    "pred = clf.predict(reviews_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This movie was excellent' => pos\n",
      "'Absolute joy ride' => pos\n",
      "'Steven Seagal was terrible' => neg\n",
      "'Steven Seagal shined through.' => neg\n",
      "'This was certainly a movie' => neg\n",
      "'Two thumbs up' => neg\n",
      "'I fell asleep halfway through' => neg\n",
      "\"We can't wait for the sequel!!\" => neg\n",
      "'!' => neg\n",
      "'?' => neg\n",
      "'I cannot recommend this highly enough' => pos\n",
      "'instant classic.' => pos\n",
      "'Steven Seagal was amazing. His performance was Oscar-worthy.' => neg\n"
     ]
    }
   ],
   "source": [
    "# print out results\n",
    "for review, category in zip(reviews_new, pred):\n",
    "    print('%r => %s' % (review, movie_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "import pprint\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'neg', u'pos']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)\n",
    "            ]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the list of all words to store the most frequently occuring ones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u',', 77717), (u'the', 76529), (u'.', 65876), (u'a', 38106), (u'and', 35576), (u'of', 34123), (u'to', 31937), (u\"'\", 30585), (u'is', 25195), (u'in', 21822), (u's', 18513), (u'\"', 17612), (u'it', 16107), (u'that', 15924), (u'-', 15595), (u')', 11781), (u'(', 11664), (u'as', 11378), (u'with', 10792), (u'for', 9961)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making a frequency distribution of the words\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "all_words.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[\"hate\"]  ## counting the occurences of a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train first 5000 words on the list\n",
    "feature_words = list(all_words.keys())[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    feature = {}\n",
    "    for w in feature_words:\n",
    "        feature[w] = (w in words)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training the classifier\n",
    "training_set = feature_sets[:1900]\n",
    "testing_set = feature_sets[1900:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TO-DO: To build own naive bais algorithm\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Naive bayes classifier accuracy percentage : ', 70.0)\n"
     ]
    }
   ],
   "source": [
    "## Testing it's accuracy\n",
    "print(\"Naive bayes classifier accuracy percentage : \", (nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               insulting = True              neg : pos    =     11.1 : 1.0\n",
      "                  doubts = True              pos : neg    =      9.6 : 1.0\n",
      "                    sans = True              neg : pos    =      8.4 : 1.0\n",
      "             wonderfully = True              pos : neg    =      8.4 : 1.0\n",
      "                    scum = True              pos : neg    =      8.2 : 1.0\n",
      "              mediocrity = True              neg : pos    =      7.7 : 1.0\n",
      "                   tripe = True              neg : pos    =      7.7 : 1.0\n",
      "               overboard = True              pos : neg    =      7.6 : 1.0\n",
      "            coincidences = True              neg : pos    =      7.1 : 1.0\n",
      "               dismissed = True              pos : neg    =      6.9 : 1.0\n",
      "                    taxi = True              pos : neg    =      6.5 : 1.0\n",
      "             bruckheimer = True              neg : pos    =      6.4 : 1.0\n",
      "                 wasting = True              neg : pos    =      6.4 : 1.0\n",
      "                  wasted = True              neg : pos    =      6.3 : 1.0\n",
      "                  fabric = True              pos : neg    =      6.3 : 1.0\n",
      "             overwhelmed = True              pos : neg    =      6.3 : 1.0\n",
      "               uplifting = True              pos : neg    =      6.1 : 1.0\n",
      "                    wits = True              pos : neg    =      5.6 : 1.0\n",
      "                    lang = True              pos : neg    =      5.6 : 1.0\n",
      "                 topping = True              pos : neg    =      5.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is how the Naive Bayes classifier expects the input\n",
    "def create_word_features(words):\n",
    "    \n",
    "    # Remove all stopwords\n",
    "    useful_words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # For each word, we create a dictionary with all the words and True. \n",
    "    # Why a dictionary? So that words are not repeated. If a word already exists, it won’t be added to the dictionary.\n",
    "    my_dict = dict([(word, True) for word in useful_words])\n",
    "    \n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': True, 'better': True, 'r': True, 'java': True}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_word_features([\"python\", \"is\", \"better\", \"than\", \"r\", \"and\", \"r\", \"is\", \"better\", \"than\", \"java\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create an empty list called neg_reviews. Next, we loop over all the files in the neg folder.\n",
    "neg_reviews = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    # We get all the words in that file.\n",
    "    words = movie_reviews.words(fileid)\n",
    "    # Then we use the function we wrote earlier to create word features in the format nltk expects.\n",
    "    neg_reviews.append((create_word_features(words), \"negative\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({u'concept': True, u'-': True, u'insight': True, u'salvation': True, u'playing': True, u'executed': True, u'go': True, u'still': True, u'find': True, u'seemed': True, u'member': True, u'touches': True, u'thrilling': True, u'craziness': True, u'somewhere': True, u'(': True, u'excites': True, u'seems': True, u',': True, u'snag': True, u'presents': True, u'going': True, u'4': True, u'pretty': True, u'skip': True, u'folks': True, u'8': True, u'main': True, u'might': True, u'good': True, u'7': True, u'get': True, u'big': True, u'showing': True, u'continues': True, u'watch': True, u'break': True, u'feels': True, u'every': True, u'know': True, u'half': True, u'world': True, u'bit': True, u'password': True, u'exact': True, u'dreams': True, u'cool': True, u'entire': True, u'like': True, u'lost': True, u'always': True, u'dig': True, u'wrapped': True, u'bad': True, u'highway': True, u'arrow': True, u'meantime': True, u'rarely': True, u'giving': True, u'looooot': True, u'mean': True, u'flick': True, u'neighborhood': True, u'deal': True, u'people': True, u'generation': True, u'crow': True, u'idea': True, u'dead': True, u'engaging': True, u'see': True, u'decided': True, u'video': True, u'wes': True, u'critique': True, u'happen': True, u'guess': True, u'really': True, u'even': True, u'given': True, u'throughout': True, u'hide': True, u\"'\": True, u'bottom': True, u'ways': True, u'movie': True, u'review': True, u'since': True, u'audience': True, u'/': True, u'applaud': True, u'3': True, u'unravel': True, u'melissa': True, u'correctly': True, u'assuming': True, u'got': True, u'flicks': True, u'ever': True, u'?': True, u'shows': True, u'lazy': True, u'nightmare': True, u'turning': True, u'packaged': True, u'little': True, u'understanding': True, u'nightmares': True, u'completely': True, u'disappearances': True, u'studio': True, u'although': True, u'others': True, u'genre': True, u'20': True, u'sitting': True, u'memento': True, u'accident': True, u'package': True, u'starts': True, u'elm': True, u'terribly': True, u'american': True, u'entertain': True, u'witch': True, u'weird': True, u'runtime': True, u'head': True, u'jumbled': True, u'chopped': True, u'movies': True, u'strangeness': True, u'point': True, u'character': True, u'explained': True, u'whatever': True, u'one': True, u'fantasy': True, u'tons': True, u'simply': True, u'church': True, u'actually': True, u'visions': True, u'plot': True, u'fed': True, u'\"': True, u'sad': True, u'bentley': True, u'would': True, u'&': True, u'decent': True, u'apparently': True, u'two': True, u'.': True, u'start': True, u'secret': True, u'production': True, u'music': True, u'way': True, u'films': True, u':': True, u'final': True, u'teen': True, u'slasher': True, u'life': True, u'kids': True, u'offering': True, u'suits': True, u'explanation': True, u'figured': True, u'took': True, u'new': True, u'back': True, u'taken': True, u'part': True, u'stick': True, u'personally': True, u'chasing': True, u'line': True, u'trying': True, u'despite': True, u'joblo': True, u'10': True, u'kind': True, u'echoes': True, u'look': True, u'plain': True, u'oh': True, u'harder': True, u'scenes': True, u'us': True, u'apparitions': True, u'2': True, u'street': True, u'neat': True, u'making': True, u'problem': True, u'feeling': True, u'minutes': True, u'blair': True, u'overall': True, u'give': True, u'unraveling': True, u'mind': True, u'fuck': True, u'biggest': True, u'couples': True, u'need': True, u'someone': True, u'shelves': True, u'want': True, u'sense': True, u'girlfriend': True, u'seem': True, u'mold': True, u'sagemiller': True, u'film': True, u'chase': True, u'!': True, u'different': True, u'kudos': True, u')': True, u'things': True, u'make': True, u'holds': True, u'generally': True, u'write': True, u'also': True, u'hot': True, u'strange': True, u'actors': True, u'9': True, u'party': True, u'confusing': True, u'stir': True, u'horror': True, u'sure': True, u'okay': True, u'beauty': True, u'normal': True, u'mess': True, u'dies': True, u'drink': True, u'problems': True, u'director': True, u'running': True, u'characters': True, u'coming': True, u'clue': True, u'guys': True, u'types': True, u'ago': True, u'downshifts': True, u'attempt': True, u'ending': True, u'makes': True, u'well': True, u'drive': True, u'redundant': True, u'obviously': True, u'sorta': True, u'edge': True, u'enter': True, u'five': True, u'entertaining': True, u'years': True, u'away': True, u'came': True}, 'negative')\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(neg_reviews[0])    \n",
    "print(len(neg_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let’s do the same for the positive reviews. The code is exactly the same:\n",
    "pos_reviews = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    pos_reviews.append((create_word_features(words), \"positive\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({u'childs': True, u'steve': True, u'surgical': True, u'/': True, u'go': True, u'certainly': True, u'watchmen': True, u'song': True, u'simpsons': True, u'novel': True, u'jack': True, u'surgeon': True, u'level': True, u'turns': True, u'michael': True, u'flashy': True, u'sooty': True, u'direct': True, u'past': True, u'street': True, u'design': True, u'befriends': True, u'odd': True, u'even': True, u'new': True, u'supporting': True, u'never': True, u'quell': True, u'les': True, u'102': True, u'strong': True, u'great': True, u'kids': True, u'30': True, u'creepy': True, u'nervous': True, u'rafael': True, u'named': True, u'love': True, u'brought': True, u'color': True, u'ians': True, u'would': True, u'indians': True, u'chooses': True, u'music': True, u'films': True, u'oscar': True, u'holm': True, u'arthouse': True, u'keeping': True, u'graphic': True, u'word': True, u'car': True, u'era': True, u'crazy': True, u'coltrane': True, u'carrot': True, u'sense': True, u'needs': True, u'end': True, u'holds': True, u'copious': True, u'worry': True, u'pales': True, u'screenwriters': True, u'tries': True, u'mad': True, u'1888': True, u'filthy': True, u'bleak': True, u'attempt': True, u'cloaking': True, u'imagining': True, u'acts': True, u'finalized': True, u'unfortunates': True, u'course': True, u'london': True, u'ghastly': True, u'hell': True, u'apes': True, u'police': True, u'interesting': True, u',': True, u'actually': True, u'better': True, u'production': True, u'hidden': True, u'might': True, u'good': True, u'killer': True, u'half': True, u'arriving': True, u'viewers': True, u'robbie': True, u'stiff': True, u'flashbacks': True, u'series': True, u'crimes': True, u'society': True, u'books': True, u'really': True, u'jackson': True, u'opened': True, u\"'\": True, u'bad': True, u'content': True, u'acting': True, u'print': True, u'surprise': True, u'turning': True, u'theory': True, u'geared': True, u'completely': True, u'language': True, u'created': True, u'could': True, u'days': True, u'british': True, u'times': True, u'thing': True, u'place': True, u'guttenberg': True, u'footnotes': True, u'onto': True, u'think': True, u'first': True, u'copper': True, u'features': True, u'one': True, u'another': True, u'comic': True, u'researched': True, u'little': True, u'\"': True, u'top': True, u'accent': True, u'anyone': True, u'2': True, u'white': True, u'stomach': True, u'godley': True, u'albert': True, u'alan': True, u'part': True, u'blindly': True, u'gruesome': True, u'rables': True, u'12': True, u'roles': True, u'bother': True, u'scenes': True, u'victorian': True, u'r': True, u'toward': True, u'slay': True, u'say': True, u'prophetic': True, u'allen': True, u'saw': True, u'superman': True, u'thoroughly': True, u'-': True, u'mid': True, u'ably': True, u'performance': True, u'mis': True, u'electric': True, u'dalmatians': True, u'though': True, u'mouth': True, u'plenty': True, u'nothing': True, u'pages': True, u'surprising': True, u'frederick': True, u'typically': True, u'saying': True, u'ending': True, u'deftly': True, u'find': True, u'(': True, u'winner': True, u'enough': True, u'crime': True, u'starters': True, u'surroundings': True, u'black': True, u'get': True, u'watch': True, u'nearly': True, u'words': True, u'ghetto': True, u'yglesias': True, u'whitechapel': True, u'investigate': True, u'briefed': True, u'abberline': True, u'looks': True, u'set': True, u'dreariness': True, u'prague': True, u'deming': True, u'see': True, u'locals': True, u'subject': True, u'00': True, u'manson': True, u'horribly': True, u'behind': True, u'finger': True, u'vertical': True, u'however': True, u'killing': True, u'blame': True, u'job': True, u'joe': True, u'marilyn': True, u'drug': True, u'geniuses': True, u'whole': True, u'riddle': True, u'point': True, u'reasons': True, u'hollow': True, u'twin': True, u'capable': True, u'particulars': True, u'secret': True, u'much': True, u'mary': True, u'500': True, u'dismiss': True, u'precision': True, u'finished': True, u'stonecutters': True, u'case': True, u'hughes': True, u'handling': True, u'look': True, u'solid': True, u'appearance': True, u'hayes': True, u'sexuality': True, u'depp': True, u'spawn': True, u'almost': True, u'ii': True, u'helped': True, u'inspector': True, u'superheroes': True, u'funny': True, u'absinthe': True, u')': True, u'make': True, u'unfortunate': True, u'80s': True, u'peaks': True, u'upon': True, u'cinematographer': True, u'identity': True, u'whores': True, u'adapted': True, u'well': True, u'kelly': True, u'captures': True, u'ludicrous': True, u'sleepy': True, u'violent': True, u'casper': True, u'campbell': True, u'blow': True, u'seems': True, u'source': True, u'crack': True, u'east': True, u'casting': True, u'big': True, u'dark': True, u'dreamy': True, u'world': True, u'unique': True, u'dreams': True, u'like': True, u'success': True, u'remind': True, u'martin': True, u'jews': True, u'crowd': True, u'back': True, u'eddie': True, u'peter': True, u'committing': True, u'comments': True, u'shakespeare': True, u'?': True, u'unsuccessfully': True, u'ripper': True, u'comparison': True, u'opium': True, u'anything': True, u'getting': True, u'violence': True, u'block': True, u'profession': True, u'widower': True, u'carving': True, u'burton': True, u'.': True, u'log': True, u'question': True, u'long': True, u':': True, u'medium': True, u'batman': True, u'mysterious': True, u'gore': True, u'made': True, u'consist': True, u'whether': True, u'stumbling': True, u'planet': True, u'limit': True, u'gould': True, u'called': True, u'irish': True, u'moore': True, u'johnny': True, u'cringed': True, u'psychopath': True, u'graham': True, u'proceeds': True, u'film': True, u'heather': True, u'englishman': True, u'amounts': True, u'tim': True, u'book': True, u'star': True, u'brothers': True, u'whistling': True, u'includes': True, u'terry': True, u'ghost': True, u'calls': True, u'directors': True, u'time': True, u'richardson': True, u'menace': True, u'starting': True, u'original': True}, 'positive')\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(pos_reviews[0])    \n",
    "print(len(pos_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 500)\n"
     ]
    }
   ],
   "source": [
    "# We will now create our test and train samples\n",
    "train_set = neg_reviews[:750] + pos_reviews[:750]\n",
    "test_set =  neg_reviews[750:] + pos_reviews[750:]\n",
    "\n",
    "print(len(train_set),  len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.4\n"
     ]
    }
   ],
   "source": [
    "# Let’s create our Naive Bayes Classifier, and train it with our training set.\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# And let’s use our test set to find the accuracy\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "It would be impossible to sum up all the stuff that sucks about this film, so I'll break it down into what I remember most strongly: a man in an ingeniously fake-looking polar bear costume (funnier than the \"bear\" from Hercules in New York); an extra with the most unnatural laugh you're ever likely to hear; an ex-dope addict martian with tics; kid actors who make sure every syllable of their lines are slowly and caaarreee-fulll-yyy prrooo-noun-ceeed; a newspaper headline stating that Santa's been \"kidnaped\", and a giant robot. Yes, you read that right. A giant robot.\n",
      "\n",
      "The worst acting job in here must be when Mother Claus and her elves have been \"frozen\" by the \"Martians'\" weapons. Could they be *more* trembling? I know this was the sixties and everyone was doped up, but still.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_santa = '''\n",
    "\n",
    "It would be impossible to sum up all the stuff that sucks about this film, so I'll break it down into what I remember most strongly: a man in an ingeniously fake-looking polar bear costume (funnier than the \"bear\" from Hercules in New York); an extra with the most unnatural laugh you're ever likely to hear; an ex-dope addict martian with tics; kid actors who make sure every syllable of their lines are slowly and caaarreee-fulll-yyy prrooo-noun-ceeed; a newspaper headline stating that Santa's been \"kidnaped\", and a giant robot. Yes, you read that right. A giant robot.\n",
    "\n",
    "The worst acting job in here must be when Mother Claus and her elves have been \"frozen\" by the \"Martians'\" weapons. Could they be *more* trembling? I know this was the sixties and everyone was doped up, but still.\n",
    "'''\n",
    "print(review_santa )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(review_santa)\n",
    "words = create_word_features(words)\n",
    "classifier.classify(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spirited Away' is the first Miyazaki I have seen, but from this stupendous film I can tell he is a master storyteller. A hallmark of a good storyteller is making the audience empathise or pull them into the shoes of the central character. Miyazaki does this brilliantly in 'Spirited Away'. During the first fifteen minutes we have no idea what is going on. Neither does the main character Chihiro. We discover the world as Chihiro does and it's truly amazing to watch. But Miyazaki doesn't seem to treat this world as something amazing. The world is filmed just like our workaday world would. The inhabitants of the world go about their daily business as usual as full with apathy as us normal folks. Places and buildings are not greeted by towering establishing shots and majestic music. The fact that this place is amazing doesn't seem to concern Miyazaki.\n",
      "\n",
      "What do however, are the characters. Miyazaki lingers upon the characters as if they were actors. He infixes his animated actors with such subtleties that I have never seen, even from animation giants Pixar. Twenty minutes into this film and I completely forgot these were animated characters; I started to care for them like they were living and breathing. Miyazaki treats the modest achievements of Chihiro with unashamed bombast. The uplifting scene where she cleanses the River God is accompanied by stirring music and is as exciting as watching gladiatorial combatants fight. Of course, by giving the audience developed characters to care about, the action and conflicts will always be more exciting, terrifying and uplifting than normal, generic action scenes. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_spirit = '''\n",
    "Spirited Away' is the first Miyazaki I have seen, but from this stupendous film I can tell he is a master storyteller. A hallmark of a good storyteller is making the audience empathise or pull them into the shoes of the central character. Miyazaki does this brilliantly in 'Spirited Away'. During the first fifteen minutes we have no idea what is going on. Neither does the main character Chihiro. We discover the world as Chihiro does and it's truly amazing to watch. But Miyazaki doesn't seem to treat this world as something amazing. The world is filmed just like our workaday world would. The inhabitants of the world go about their daily business as usual as full with apathy as us normal folks. Places and buildings are not greeted by towering establishing shots and majestic music. The fact that this place is amazing doesn't seem to concern Miyazaki.\n",
    "\n",
    "What do however, are the characters. Miyazaki lingers upon the characters as if they were actors. He infixes his animated actors with such subtleties that I have never seen, even from animation giants Pixar. Twenty minutes into this film and I completely forgot these were animated characters; I started to care for them like they were living and breathing. Miyazaki treats the modest achievements of Chihiro with unashamed bombast. The uplifting scene where she cleanses the River God is accompanied by stirring music and is as exciting as watching gladiatorial combatants fight. Of course, by giving the audience developed characters to care about, the action and conflicts will always be more exciting, terrifying and uplifting than normal, generic action scenes. \n",
    "'''\n",
    "print(review_spirit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(review_spirit)\n",
    "words = create_word_features(words)\n",
    "classifier.classify(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
